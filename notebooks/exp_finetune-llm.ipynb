{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31df35f6",
   "metadata": {},
   "source": [
    "# Fine-Tuned Historian (Structure Aware Generation - Storytelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f35694",
   "metadata": {},
   "source": [
    "Here we finetume TinyLlama on TinyStories dataset to see if we can make a better historian for our multimodal systems. The big question is would it be better/worse than using a dictionary memory or is there a reasonable use case for both?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c830798",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7629ee78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac6ea00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Tim and Mia like to play in the park. They see a big club on the ground. It is brown and long and heavy.\\n\\n\"Look, a club!\" Tim says. \"I can lift it!\"\\n\\nHe tries to lift the club, but it is too tough. He falls down and drops the club.\\n\\n\"Ouch!\" he says. \"That hurt!\"\\n\\nMia laughs. She is not mean, she just thinks it is funny.\\n\\n\"Let me try!\" she says. \"I can balance it!\"\\n\\nShe picks up the club and puts it on her head. She walks slowly and carefully. She does not fall down.\\n\\n\"Wow!\" Tim says. \"You are good at balancing!\"\\n\\n\"Thank you!\" Mia says. \"It is fun!\"\\n\\nThey take turns balancing the club on their heads, arms, and legs. They have a lot of fun with the club. They are happy and proud. They are good friends.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", cache_dir=\"../.cache\")\n",
    "dataset = dataset.shuffle(seed=42).select(range(10000))\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "838e2625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168deaf",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86a8f2",
   "metadata": {},
   "source": [
    "Tokenize the stories using TinyLlama's tokenizer (ensures alignment with the target model‚Äôs input expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ae9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26592f",
   "metadata": {},
   "source": [
    "The tokenizer converts raw text into a sequence of tokens (subwords). For example \"Hello, world!\" might be tokenized into [\"Hello\", \",\", \"world\", \"!\"]. \n",
    "\n",
    "THe tokenizer object here also encodes the tokens with tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a7c0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokens = tokenizer(\n",
    "        batch[\"text\"], truncation=True, padding=\"max_length\", max_length=512\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # üéØ Use same tokens as targets\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c55b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc04c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# make sure dataset returns labels: 'input_ids', 'attention_mask', 'labels'\n",
    "print(tokenized_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52fcfa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32000\n",
      "Model input names: ['input_ids', 'attention_mask']\n",
      "['‚ñÅHello', '!']\n",
      "[10994, 29991]\n",
      "{'input_ids': tensor([[    1, 15043,  3186, 29991]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(\n",
    "    f\"Model input names: {tokenizer.model_input_names}\"\n",
    ")  # this is a list of keys that the model expects so the model can consume it\n",
    "# Example usage of the tokenizer\n",
    "print(tokenizer.tokenize(\"Hello!\"))\n",
    "print(tokenizer.convert_tokens_to_ids([\"Hello\", \"!\"]))\n",
    "# Example usage of the tokenizer\n",
    "print(tokenizer(\"Hello world!\", return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e9832",
   "metadata": {},
   "source": [
    "The model object contains all of the model's parameters and architecture.\n",
    "\n",
    "In this case we are working with a decoder-only transformer (processes text left to right and predicts the next token at each step).\n",
    "\n",
    "input_ids ‚Üí embed_tokens ‚Üí [x22 LlamaDecoderLayer] ‚Üí norm ‚Üí lm_head ‚Üí logits\n",
    "\n",
    "Where each LlamaDecoderLayer does (attention ‚Üí norm) ‚Üí (MLP ‚Üí norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "873e77f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print(model.config)\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfbf2bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1,100,048,384\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4504db0",
   "metadata": {},
   "source": [
    "So, we are working with a 1.1B parameter model and a dataset of 10k stories on fairly minimal hardware. \n",
    "\n",
    "| Data-to-Parameter Ratio  | Recommended Strategy                    |\n",
    "|--------------------------|-----------------------------------------|\n",
    "| **1 : 1,000,000+**        | Use **LoRA** or adapters only           |\n",
    "| **1 : 100,000 ‚Äì 1,000,000** | Freeze most layers, tune only top few  |\n",
    "| **1 : 1,000 ‚Äì 100,000**   | Partial unfreeze (e.g., last 2‚Äì4 layers)|\n",
    "| **1 : <1,000**            | Full fine-tuning is feasible            |\n",
    "\n",
    "SO rule of thumb says we'd probably want to freeze most of the layers or use LoRA since our data-to-parameter ratio is **1 : 110,000**...let's try it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaae3e5",
   "metadata": {},
   "source": [
    "## Fine-Tuning using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca3df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # size of the low-rank matrices\n",
    "    lora_alpha=32,  # scaling factor for the LoRA updates\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # apply LoRA only to these layers\n",
    "    lora_dropout=0.05,  # dropout rate for the LoRA layers\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d7e76a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7555eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['loss', 'logits', 'past_key_values'])\n"
     ]
    }
   ],
   "source": [
    "# test: confirm that the model accepts and computes loss\n",
    "sample = tokenized_dataset[0]\n",
    "input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0)\n",
    "attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0)\n",
    "labels = torch.tensor(sample[\"labels\"]).unsqueeze(0)\n",
    "\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "print(output.keys())\n",
    "# Should include 'loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f53f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/4zf8zzj14kx2rpg0wjq7rdh80000gn/T/ipykernel_23383/2588874555.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../.cache/historian_model\",  # where to save the model\n",
    "    # batch size controls the number of samples processed in parallel...bigger batches have more stable gradients but require more memory so more GPU power would allow larger batches. Since we are working on a CPU/low memory situation and its a fairly small model we'll try 1-4\n",
    "    per_device_train_batch_size=4,\n",
    "    # since we are using a pretrained model and small dataset (and no GPU), 1-3 epoch for fine-tuning are probably enough. Starting with 1 for quick test/diagnostics run is good\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,  # how often to log training progress\n",
    "    save_steps=500,  # how often to save the model\n",
    "    fp16=False,  # use mixed precision training for faster training on GPUs\n",
    "    save_total_limit=1,  # keep only the latest model checkpoint\n",
    "    report_to=\"none\",  # disable reporting to any logging service\n",
    "    # max_steps=300,  # past runs show that loss levels off after ~300 steps, so we can limit training to that (or use early stopping callback below)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3)\n",
    "    ],  # stop training if no improvement after 3 steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e9a9e6",
   "metadata": {},
   "source": [
    "Lets do a quick dry run time estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then multiply this by total steps to get a time estimate\n",
    "# trainer.train(max_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60130489",
   "metadata": {},
   "source": [
    "Great so now we are training and we can see the progress...\n",
    "\n",
    "number of step is total examples / batch size\n",
    "\n",
    "- the epochs are one full pass through the whole dataset\n",
    "- the steps are the batches\n",
    "\n",
    "[current step of / 2500 steps total, time elapsed < estimated time, iteration speed (steps/sec), epoch (current/total)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d30cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yarik/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1410' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1410/2500 47:44 < 36:57, 0.49 it/s, Epoch 0.56/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.639800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.676100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.641600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.641900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.639800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.601900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.613200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.621800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.627600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.589200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.601300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.615900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.607600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.602800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.596400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.597000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.576500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2548\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2549\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2553\u001b[0m )\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2561\u001b[0m ):\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/trainer.py:3745\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3744\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3745\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3747\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3750\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3751\u001b[0m ):\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/trainer.py:3810\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3809\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3810\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/peft/peft_model.py:1757\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1756\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1757\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1768\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    684\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    685\u001b[0m )\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:308\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:265\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m         attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[0;32m--> 265\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    277\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py:54\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(is_causal, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     52\u001b[0m     is_causal \u001b[38;5;241m=\u001b[39m is_causal\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 54\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    # resume_from_checkpoint=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711ad692",
   "metadata": {},
   "source": [
    "Loss pretty much leveled out so we can stop training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76991dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxUAAAHqCAYAAAByRmPvAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUYNJREFUeJzt3Ql4VOXZ//F7sieEJCyyrwoiiyjiArigr2zii6DWttgKtmrrWq1rra2CvBatK1YFlwpqxfVfsFoUEAVEQQUBgSqCIqCyFCEJ2SfJ+V/3k8yQhCwDycxzzsz3c12TmTlzZubkPLOc3zybz3EcRwAAAADgMMUd7h0BAAAAQBEqAAAAADQKoQIAAABAoxAqAAAAADQKoQIAAABAoxAqAAAAADQKoQIAAABAoxAqAAAAADQKoQIAAABAoxAqAMBjLr30UunWrdth3XfSpEni8/mafJsAALGNUAEATUQP1kM5LV68WGI1DKWnp4tXzJkzR8455xxp3bq1JCUlSYcOHeSnP/2pvPfee7Y3DQBcx+c4jmN7IwAgGvzjH/+odv3555+XhQsXygsvvFBt+fDhw6Vt27aH/Tx+v1/Ky8slOTn5kO9bWlpqTikpKWIjVLz++uuSl5cnbqZfi7/+9a9l1qxZMmDAAPnJT34i7dq1kx07dpigsWrVKvnwww9lyJAhtjcVAFwjwfYGAEC0+OUvf1nt+ooVK0yoqLm8poKCAklLSwv5eRITEw97GxMSEswJdXvwwQdNoLjhhhvkoYceqtZc7I477jAhsSn2oYaXoqIiSU1NbfRjAYBtNH8CgAg688wzpV+/fubX7jPOOMOEiT/+8Y/mtjfeeEPOPfdc08xGayGOOuoomTJlipSVldXbp+Lbb781B74PPPCAPPXUU+Z+ev+TTjpJPv300wb7VOj1a6+9VubOnWu2Te/bt29feeeddw7afm26deKJJ5qaDn2eJ598ssn7abz22msycOBAc7CtTY80lH3//ffV1tm5c6f86le/kk6dOpntbd++vYwdO9bsi4CVK1fKyJEjzWPoY3Xv3t3UQNSnsLBQpk6dKsccc4zZn7X9X5dccomcfPLJ5nJd/7uGEl1edXu0zP73f/9X5s+fb/ahbpPuP93nZ5111kGPobVRHTt2NDUlVZc98sgjpny0DLTG67e//a3s27evwf0KAOHEz1UAEGE//vijaav/85//3BwwB5pC6YGo9jm48cYbzbm23b/zzjslNzdX7r///gYfd/bs2bJ//35zkKkHtH/961/lggsukG+++abB2o1ly5bJP//5T7n66qulefPm8uijj8qFF14o27Ztk1atWpl1Vq9eLaNGjTIH8JMnTzZh5+6775YjjjiiifZMxT7QsKCBSA/ud+3aJdOmTTPNjfT5s7KyzHq6bRs2bJDrrrvOHKzv3r3b1Arp9gaujxgxwmzbH/7wB3M/PcDX/7Gh/bB3715TSxEfHy9NbePGjTJ+/HhTRldccYX06tVLfvazn5lwokFJm1lV3ZYffvjBvE4C9H6BffS73/1OtmzZIo899pjZN7qPGlOLBQCNon0qAABN75prrtE+a9WWDR061CybMWPGQesXFBQctOy3v/2tk5aW5hQVFQWXTZw40enatWvw+pYtW8xjtmrVytm7d29w+RtvvGGWv/nmm8Fld91110HbpNeTkpKczZs3B5etXbvWLP/b3/4WXDZmzBizLd9//31w2aZNm5yEhISDHrM2ut3NmjWr8/aSkhKnTZs2Tr9+/ZzCwsLg8rfeess8/p133mmu79u3z1y///7763ysOXPmmHU+/fRT51BMmzbN3E/vH4ra9qeaOXOmWa5lE6Blpsveeeedautu3LjxoH2trr76aic9PT34uvjggw/Mei+++GK19fTxalsOAJFE8ycAiDBtrqO/NNdUtW291jjs2bNHTj/9dNPn4ssvv2zwcfUX7xYtWgSv632V1lQ0ZNiwYaY5U0D//v0lIyMjeF+tlXj33Xdl3LhxpnlWQI8ePUytS1PQ5kpaw6C1JVU7kmuTMG2O9O9//zu4n3Q0Jm2KVVezn0CNxltvvWU6todKa4WU1taEgzbB0iZZVR199NFy/PHHyyuvvBJcpvtbO7WPGTMm+LrQZmGZmZmmo7++NgInbSqmNVvvv/9+WLYZAEJBqACACNN28npQXJM25zn//PPNgaMe0GvTnUAn75ycnAYft0uXLtWuBwJGKO3ta943cP/AffVgX/sbaIioqbZlh2Pr1q3mXJsE1aShInC7hrL77rtP3n77bdN0TPumaFMvbT4UMHToUNNESptpaZ8K7W8xc+ZMKS4urncbdL8HQl24QkVdgVCbLwX6jmhg0n2uywM2bdpkXgdt2rQxr42qJx1RS9cHAFsIFQAQYbWN9pOdnW0OhNeuXWv6Kbz55pumj4AePAc66Dakrj4AoYwc3pj72qB9Hr766ivT70JrNf785z9L7969Td8CpX1K9Jf+5cuXm07oerCunbT1V/36hrTV8KLWrVsX0nbU1UG9Zuf6gLpGetLwoPtaayPUq6++asKl9mEJ0NeABgp9XdR20tcNANhCqAAAF9BfprUDt3bCvf76680oQdokqWpzJpv0YFYP3jdv3nzQbbUtOxxdu3YNdmauSZcFbg/Q5lo33XSTLFiwQNavXy8lJSVmONiqBg0aJPfcc49pWvXiiy+a2qCXX365zm047bTTzD5/6aWX6gwGVQXKR0NhVYFalUOpwdARpbQJlM4joh3KtalZ1blI9P/V18ipp55qXhs1T8cdd9whPScANCVCBQC4QKCmoGrNgB4kP/HEE+KW7dMDVx12VkckqhootBlSU9BhVjW8zJgxo1ozJX38L774wvStUNrHROd3qEoPuLUfROB+2myrZi2L9ltQ9TWB0iF+b7vtNvN8el5bTY1OcvjJJ58En1ctXbo0eHt+fr4899xzh/z/a22Fzm3y7LPPmr4SVZs+KZ3NW4OODjNckwaRmsEGACKJIWUBwAV0dmb91XvixIlmqFBtVqOTrLmp+ZEOe6q1AvpL+VVXXWUOcHU4U51nYc2aNSE9hnaa/r//+7+Dlrds2dJ00NbmXtqJXZuC6dCrgSFldZjY3//+92ZdbfZ09tlnm4PsPn36mInodKZrXTcw/Koe1Gsg0z4qeuCvfSSefvpp02di9OjR9W7jLbfcYmo0tNZDOz8HZtTWPhsaqjRQfPTRR2ZdHbZW+6Ncdtll5n4avjQUaD8HHd72UOj/c/PNN5uT7g8NcVXpPtEhZbXJl+5vfW4dQlb7WmizKd1PVee0AIBIIlQAgAvoXBA6UpE25/nTn/5kAoZ20taD55qjBdmi/RG01kAPerUPQ+fOnU07fv1VP5TRqQK1L3rfmvTAX0OFTuyntQX33nuvqSlo1qyZCQYaNgIjOunzauBYtGhRcHZr7Quh/RC0c3bgAFwP/rWpk4YN7Z+gzYu0CVRdnaUD4uLi5Pnnnzedu3UyQZ0ET0eF0qAQ6BQ+ePBgs64e1Gug0W3X/0vDh/b30PKrbYSv+uhEfhoutcP25ZdfXuucE1qLo+Wgk+bppIn6v2vg0teKhj0AsMWn48pae3YAgOdp23/9ZV9/MQcAxCb6VAAAQqbDylalQWLevHly5plnWtsmAIB91FQAAELWvn1700TpyCOPNCMcTZ8+3XR81qFce/bsaXvzAACW0KcCABAynTdBh1vVTss63Kn2LfjLX/5CoACAGEdNBQAAAIBGoU8FAAAAgEYhVAAAAABolJjrU1FeXm5mg9WZV3VyKQAAAAC1054SOoFohw4dzDw+dYm5UKGBQidOAgAAABCa7du3m0k66xJzoUJrKAI7JiMjo9Z1/H6/LFiwQEaMGFHrjKYIP8rALva/fZSBfZSBfZSBfZSBXX4X7P/c3Fzzg3zgGLouMRcqAk2eNFDUFyrS0tLM7byB7KAM7GL/20cZ2EcZ2EcZ2EcZ2OV30f5vqNsAHbUBAAAANAqhAgAAAECjECoAAAAANAqhAgAAAECjECoAAAAANAqhAgAAAECjECoAAAAANAqhAgAAAECjECoAAAAANAqhAgAAAECjECoAAAAANEpC4+6OQ1VW7sgnW/bK7v1F0qZ5ipzcvaXEx/lsbxYAAABw2AgVEfTO+h0y+c3/yI6couCy9pkpcteYPjKqX3ur2wYAAAAcLpo/RTBQXPWPz6oFCrUzp8gs19sBAAAALyJURKjJk9ZQOLXcFlimt+t6AAAAgNcQKiJA+1DUrKGoSqOE3q7rAQAAAF5DqIgA7ZTdlOsBAAAAbkKoiAAd5akp1wMAAADchFARATpsrI7yVNfAsbpcb9f1AAAAAK8hVESAzkOhw8aqmsEicF1vZ74KAAAAeBGhIkJ0HorpvzxB2mVWb+Kk13U581QAAADAq5j8LoI0OAzv004unP6hrNmeI78940i5ddQx1FAAAADA06ipiDANEN1aNTOXW6cnEygAAADgeYQKC7LSksx5dmGJ7U0BAAAAGo1QYUFGaqI5zyn0294UAAAAoNEIFRZkVYaK7AJCBQAAALyPUGFBJjUVAAAAiCKECouhIpdQAQAAgChgNVRMnz5d+vfvLxkZGeY0ePBgefvtt+tcf9asWeLz+aqdUlKqz/vgBVlplc2fCBUAAACIAlbnqejUqZPce++90rNnT3EcR5577jkZO3asrF69Wvr27VvrfTR8bNy4MXhdg4XX0PwJAAAA0cRqqBgzZky16/fcc4+pvVixYkWdoUJDRLt27cTLMtMOhIryckfimKsCAAAAHuaaGbXLysrktddek/z8fNMMqi55eXnStWtXKS8vlxNOOEH+8pe/1BlAVHFxsTkF5ObmmnO/329OtQksr+v2xkpLqAgRjiOyL68wOMQsIlcGqB/73z7KwD7KwD7KwD7KwC6/C/Z/qM/tc7TdkUXr1q0zIaKoqEjS09Nl9uzZMnr06FrXXb58uWzatMn0w8jJyZEHHnhAli5dKhs2bDBNqWozadIkmTx58kHL9XnS0tLElps/jhd/uU/uHFAqrbzXLQQAAAAxoKCgQC6++GJz7K3dEFwbKkpKSmTbtm1mQ19//XV55plnZMmSJdKnT5+QklPv3r1l/PjxMmXKlJBrKjp37ix79uypc8fo4y5cuFCGDx8uiYnhqUU47f4lsiu3WOZcOUj6day7gGJVJMoAdWP/20cZ2EcZ2EcZ2EcZ2OV3wf7XY+fWrVs3GCqsN39KSkqSHj16mMsDBw6UTz/9VKZNmyZPPvlkg/fVnTtgwADZvHlzneskJyebU233bahwQlnncGWlJplQke93eJNaKgM0jP1vH2VgH2VgH2VgH2UQu/s/McTndd08FdpXomrNQkP9MLT5VPv27cWrnbWzC0tsbwoAAADQKFZrKm6//XY555xzpEuXLrJ//37Tz2Hx4sUyf/58c/uECROkY8eOMnXqVHP97rvvlkGDBpmajezsbLn//vtl69atcvnll4vXMKwsAAAAooXVULF7924THHbs2CGZmZmmA7YGCm03prSvRVzcgcqUffv2yRVXXCE7d+6UFi1amOZSH330UUj9L9wmqzJUZBcQKgAAAOBtVkPF3//+93pv11qLqh5++GFzigaBmopcaioAAADgca7rUxEraP4EAACAaEGosCQr0FGb5k8AAADwOEKFJYFZtKmpAAAAgNcRKizJSksy59mECgAAAHgcocISOmoDAAAgWhAqLKGjNgAAAKIFocLyPBV5xaXiLyu3vTkAAADAYSNUWO6orWgCBQAAAC8jVFgSH+eT5ikVcw/SWRsAAABeRqiwiH4VAAAAiAaEChdMgJfDBHgAAADwMEKFRdRUAAAAIBoQKiwiVAAAACAaECosykytnFWb5k8AAADwMEKFRdRUAAAAIBoQKlzQUTu7sMT2pgAAAACHjVDhgpoKJr8DAACAlxEqXBAq6FMBAAAALyNUWJRFnwoAAABEAUKFRRmECgAAAEQBQoUrOmoTKgAAAOBdhAoX9KkoKS2XIn+Z7c0BAAAADguhwqL05ASJj/OZy3TWBgAAgFcRKizy+XxMgAcAAADPI1RYRqgAAACA1xEqXDNXBbNqAwAAwJsIFZZRUwEAAACvI1S4ZFhZQgUAAAC8ilBhGTUVAAAA8DpChWv6VBAqAAAA4E2ECsuoqQAAAIDXESosI1QAAADA6wgVlmWlJZnzbEIFAAAAPIpQ4ZKailxCBQAAADyKUOGSIWWZ/A4AAABeRahwS01FUak4jmN7cwAAAIBDRqhwSagoK3ckr7jU9uYAAAAAh4xQYVlKYrwkJ1QUA3NVAAAAwIsIFS7AsLIAAADwMkKFizprEyoAAADgRYQKF6CmAgAAAF5GqHBRqKBPBQAAALyIUOECmakVs2pTUwEAAAAvIlS4AM2fAAAA4GWECld11GZWbQAAAHgPocIFqKkAAACAlxEqXFRTQUdtAAAAeBGhwgUyqKkAAACAhxEqXIDmTwAAAPAyq6Fi+vTp0r9/f8nIyDCnwYMHy9tvv13vfV577TU55phjJCUlRY499liZN2+eeF1WIFTQ/AkAAAAeZDVUdOrUSe69915ZtWqVrFy5Uv7nf/5Hxo4dKxs2bKh1/Y8++kjGjx8vl112maxevVrGjRtnTuvXr5doqKnYX1wqpWXltjcHAAAA8E6oGDNmjIwePVp69uwpRx99tNxzzz2Snp4uK1asqHX9adOmyahRo+SWW26R3r17y5QpU+SEE06Qxx57TKIhVKjcolKr2wIAAAAcqgRxibKyMtO0KT8/3zSDqs3y5cvlxhtvrLZs5MiRMnfu3Doft7i42JwCcnNzzbnf7zen2gSW13V7ODRLjpf84jL5cX+BNE/ySayzUQY4gP1vH2VgH2VgH2VgH2Vgl98F+z/U57YeKtatW2dCRFFRkamlmDNnjvTp06fWdXfu3Clt27attkyv6/K6TJ06VSZPnnzQ8gULFkhaWlq927Zw4UKJlCQnXvLFJ/PeXSLdmkfsaV0vkmWAg7H/7aMM7KMM7KMM7KMMYnf/FxQUeCNU9OrVS9asWSM5OTny+uuvy8SJE2XJkiV1BotDdfvtt1er3dCais6dO8uIESNM5/C6EpkW3vDhwyUx8UDTpHCasWW57Nu5X/qdcLKc0bO1xDobZYAD2P/2UQb2UQb2UQb2UQZ2+V2w/wOtfFwfKpKSkqRHjx7m8sCBA+XTTz81fSeefPLJg9Zt166d7Nq1q9oyva7L65KcnGxONWnBNFQ4oazTVLLSksx5Xkk5b1pLZYCDsf/towzsowzsowzsowxid/8nhvi8rpunory8vFofiKq0mdSiRYuqLdP0VlcfDC/Oqs1cFQAAAPAaqzUV2jTpnHPOkS5dusj+/ftl9uzZsnjxYpk/f765fcKECdKxY0fTL0Jdf/31MnToUHnwwQfl3HPPlZdfftkMRfvUU09J1EyAx1wVAAAA8BiroWL37t0mOOzYsUMyMzPNRHgaKLTdmNq2bZvExR2oTBkyZIgJHn/605/kj3/8oxmKVkd+6tevn3hdZmVNRTY1FQAAAPAYq6Hi73//e723a61FTRdddJE5RZtgTQWhAgAAAB7juj4VsYpQAQAAAK8iVLhEVmrF6E/0qQAAAIDXECpcgpoKAAAAeBWhwmVDymYXltjeFAAAAOCQECpcgpoKAAAAeBWhwiUyKkNFkb9civxltjcHAAAACBmhwiWaJydInK/ici61FQAAAPAQQoVLxMX5grUVNIECAACAlxAqXCSrMlQwqzYAAAC8hFDhxs7azFUBAAAADyFUuEhmWsUEeNRUAAAAwEsIFS7CsLIAAADwIkKFi2SmJpjznAImwAMAAIB3ECpcJCu1ovkTNRUAAADwEkKFi9D8CQAAAF5EqHCRzDSGlAUAAID3ECpchJoKAAAAeBGhwoWT3zFPBQAAALyEUOHC5k/UVAAAAMBLCBUubf7kOI7tzQEAAABCQqhw4ZCypeWO5JeU2d4cAAAAICSEChdJSYyTpPiKIqEJFAAAALyCUOEiPp/vwLCyzKoNAAAAjyBUuAzDygIAAMBrCBVuDRUMKwsAAACPIFS4da4KaioAAADgEYQKl6H5EwAAALyGUOEywY7ahAoAAAB4BKHCZaipAAAAgNcQKtzap4KO2gAAAPAIQoVLmz9RUwEAAACvIFS4DM2fAAAA4DWECpfJTE0y59mFzKgNAAAAbyBUuAyT3wEAAMBrCBUuk1XZpyK3qFTKyh3bmwMAAAA0iFDh0poKtb+I2goAAAC4H6HCZRLj4yQtKd5czqYJFAAAADyAUOHmuSoYAQoAAAAeQKhwoQxCBQAAADyEUOHiztrZhAoAAAB4AKHChZgADwAAAF5CqHChrMoJ8HIKmAAPAAAA7keocKHMyuZP1FQAAADACwgVLkTzJwAAAHgJocLFoYJ5KgAAAOAFhAoXoqYCAAAAXkKocPGQsoQKAAAAeAGhwoWoqQAAAICXECpciD4VAAAA8BKroWLq1Kly0kknSfPmzaVNmzYybtw42bhxY733mTVrlvh8vmqnlJQUicZ5Kgr9ZVJSWm57cwAAAAD3hoolS5bINddcIytWrJCFCxeK3++XESNGSH5+fr33y8jIkB07dgRPW7dulWjSPCVBfL6KyzSBAgAAgNsl2Hzyd95556BaCK2xWLVqlZxxxhl13k9rJ9q1ayfRKi7OJxkpiSZQ5BSWyBHNk21vEgAAAOCNPhU5OTnmvGXLlvWul5eXJ127dpXOnTvL2LFjZcOGDRJt6KwNAAAAr7BaU1FVeXm53HDDDXLqqadKv3796lyvV69e8uyzz0r//v1NCHnggQdkyJAhJlh06tTpoPWLi4vNKSA3N9eca1MrPdUmsLyu2yMhM7WiaPbsL7K6Hba4oQxiGfvfPsrAPsrAPsrAPsrALr8L9n+oz+1zHMcRF7jqqqvk7bfflmXLltUaDur7R3v37i3jx4+XKVOmHHT7pEmTZPLkyQctnz17tqSlpYlbPfGfONmYEye/7FEmJx3hiiICAABAjCkoKJCLL77Y/Jiv/ZpdHSquvfZaeeONN2Tp0qXSvXv3Q77/RRddJAkJCfLSSy+FVFOhzab27NlT547RoKIdx4cPHy6JiRXNkCLt+lfWyrz1u+RPo3vJxMFdJda4oQxiGfvfPsrAPsrAPsrAPsrALr8L9r8eO7du3brBUGG1+ZPmmeuuu07mzJkjixcvPqxAUVZWJuvWrZPRo0fXentycrI51aQF01DhhLJOuLRoVrHN+4vLY/pNbLMMwP53A8rAPsrAPsrAPsogdvd/YojPazVU6HCy2gxJayl0roqdO3ea5ZmZmZKammouT5gwQTp27GjmtFB33323DBo0SHr06CHZ2dly//33myFlL7/8cokmdNQGAACAV1gNFdOnTzfnZ555ZrXlM2fOlEsvvdRc3rZtm8TFHRikat++fXLFFVeYANKiRQsZOHCgfPTRR9KnTx+JJllphAoAAAB4g/XmTw3RZlFVPfzww+YU7aipAAAAgFe4ap4KHBwqsgtKbG8KAAAAUC9ChUtlpiaZc2oqAAAA4HaECtc3fyq1vSkAAABAvQgVLnWgo3ZJSH1PAAAAAFsIFS6vqfCXOVLoL7O9OQAAAECdCBUulZYUL4nxPnM5u4B+FQAAAHAvQoVL+Xw+hpUFAACAJxAqXCwjOKwsoQIAAADuRahwsSxqKgAAAOABhAoXCzR/yiVUAAAAwMUIFS6WlVYxAV52IbNqAwAAwL0IFS5GR20AAAB4AaHCxeioDQAAAC8gVLgYHbUBAADgBYQKF6P5EwAAALyAUOFiWWmECgAAALgfocLFqKkAAACAFxAqPFBTQUdtAAAAuBmhwgOjP+UW+aW83LG9OQAAAECtCBUeaP7kOCL7i0ptbw4AAABQK0KFiyUnxEtqYry5TL8KAAAAuBWhwuXorA0AAAC3I1R4pbN2YYntTQEAAABqRajwSGdtaioAAADgVoQKjzR/YlhZAAAAuBWhwuWyqKkAAACAyxEqPFJTkUuoAAAAgEsRKlyOWbUBAADgdoQKl2NIWQAAAERlqNi+fbt89913weuffPKJ3HDDDfLUU0815bZBQ0VakjlnSFkAAABEVai4+OKL5f333zeXd+7cKcOHDzfB4o477pC77767qbcxph2oqSi1vSkAAABA04WK9evXy8knn2wuv/rqq9KvXz/56KOP5MUXX5RZs2YdzkOioVBRQE0FAAAAoihU+P1+SU5ONpffffddOe+888zlY445Rnbs2NG0WxjjGFIWAAAAURkq+vbtKzNmzJAPPvhAFi5cKKNGjTLLf/jhB2nVqlVTb2NMC9RU5JeUib+s3PbmAAAAAE0TKu677z558skn5cwzz5Tx48fLcccdZ5b/61//CjaLQtPIqAwVitoKAAAAuFHC4dxJw8SePXskNzdXWrRoEVz+m9/8RtLS0ppy+2JefJxPmqckyP6iUhMqWqdXNDsDAAAAPF1TUVhYKMXFxcFAsXXrVnnkkUdk48aN0qZNm6bexpgXaALFBHgAAACImlAxduxYef75583l7OxsOeWUU+TBBx+UcePGyfTp05t6G2NeYFbtXJo/AQAAIFpCxWeffSann366ufz6669L27ZtTW2FBo1HH320qbcx5jGrNgAAAKIuVBQUFEjz5s3N5QULFsgFF1wgcXFxMmjQIBMu0LSyUitn1WauCgAAAERLqOjRo4fMnTtXtm/fLvPnz5cRI0aY5bt375aMjIym3saYFxgBilm1AQAAEDWh4s4775Sbb75ZunXrZoaQHTx4cLDWYsCAAU29jTEv0Kciu5CaCgAAAETJkLI/+clP5LTTTjOzZwfmqFBnn322nH/++U25faBPBQAAAKIxVKh27dqZ03fffWeud+rUiYnvwh0qGFIWAAAA0dL8qby8XO6++27JzMyUrl27mlNWVpZMmTLF3IamlUVNBQAAAKKtpuKOO+6Qv//973LvvffKqaeeapYtW7ZMJk2aJEVFRXLPPfc09XbGNJo/AQAAIOpCxXPPPSfPPPOMnHfeecFl/fv3l44dO8rVV19NqGhimcGO2oQKAAAAREnzp71798oxxxxz0HJdprchfDUVjuPY3hwAAACg8aFCR3x67LHHDlquy7TGAuEJFSWl5VLkp88KAAAAoiBU/PWvf5Vnn31W+vTpI5dddpk56eVZs2bJAw88EPLjTJ06VU466SQzO3ebNm1k3LhxsnHjxgbv99prr5lakZSUFDn22GNl3rx5Es3SkxMkPs5nLtOvAgAAAFERKoYOHSpfffWVmZMiOzvbnC644ALZsGGDvPDCCyE/zpIlS+Saa66RFStWyMKFC8Xv95vZufPz8+u8z0cffSTjx483QWb16tUmiOhp/fr1Eq18Ph+dtQEAABB981R06NDhoA7Za9euNaNCPfXUUyE9xjvvvFPtutZ0aI3FqlWr5Iwzzqj1PtOmTZNRo0bJLbfcYq7rMLYaSLTp1YwZMySah5Xdm18i2QXMqg0AAIAoqKkIl5ycHHPesmXLOtdZvny5DBs2rNqykSNHmuXRLIOaCgAAAERbTUVT00nzbrjhBjPvRb9+/epcb+fOndK2bdtqy/S6Lq9NcXGxOQXk5uaac21qpafaBJbXdbsNmSkVRfVjXpGrtitc3FgGsYT9bx9lYB9lYB9lYB9lYJffBfs/1Od2TajQvhXaL0In0WtK2hl88uTJBy1fsGCBpKWl1XtfbVblFnn7tFIpTj7+7HNJ27lWYoWbyiAWsf/towzsowzsowzsowxid/8XFBQ0fajQztj10Q7bh+Paa6+Vt956S5YuXSqdOnWqd9127drJrl27qi3T67q8NrfffrvceOON1WoqOnfubDqEZ2Rk1JnItPCGDx8uiYkVzY5s+/StL2TVnu3SvlsPGT2sp0Q7N5ZBLGH/20cZ2EcZ2EcZ2EcZ2OV3wf4PtPJp0lCRmZnZ4O0TJkwI+fF0IrfrrrtO5syZI4sXL5bu3bs3eJ/BgwfLokWLTFOpAN3Zurw2ycnJ5lSTFkxDhRPKOpHSslnF/5BXXO6abYoEN5VBLGL/20cZ2EcZ2EcZ2EcZxO7+TwzxeQ8pVMycOVOausnT7Nmz5Y033jBzVQT6RWg4SU1NNZc1pHTs2NE0Y1LXX3+9GdL2wQcflHPPPVdefvllWblyZcgjTnkVHbUBAADgVlZHf5o+fboZ8enMM8+U9u3bB0+vvPJKcJ1t27bJjh07gteHDBligoiGCJ3Z+/XXX5e5c+fW27k7GmSlJZnzbEIFAAAAXMZqR21t/tQQbRZV00UXXWROsYTJ7wAAAOBWrpqnAnXLSqsMFUx+BwAAAJchVHgENRUAAABwK0KFB0NFeXnDzcYAAACASCFUeCxUaJ7IKym1vTkAAABAEKHCI1IS4yU5oaK4cgpoAgUAAAD3IFR4sbM2/SoAAADgIoQKD6GzNgAAANyIUOHBUJFN8ycAAAC4CKHCQzJTK2bVpqYCAAAAbkKo8BCaPwEAAMCNCBUe7KidXcis2gAAAHAPQoUHaypyqakAAACAixAqvFhTQUdtAAAAuAihwkPoUwEAAAA3IlR4SAZDygIAAMCFCBUekkVNBQAAAFyIUOEhdNQGAACAGxEqPCQrrWLyu/3FpVJaVm57cwAAAACDUOEhGSkJwcu5RaVWtwUAAAAIIFR4SEJ8nKQnVwSL7AImwAMAAIA7ECo8hmFlAQAA4DaECo8hVAAAAMBtCBUenVWbUAEAAAC3IFR4DDUVAAAAcBtChUdrKphVGwAAAG5BqPCYDGoqAAAA4DKECo82f6KmAgAAAG5BqPCYrNSKWbWpqQAAAIBbECo8WlORS6gAAACASxAqvNpRu5AZtQEAAOAOhAqPYUhZAAAAuA2hwmPoqA0AAAC3IVR4TGZl86fi0nIp8pfZ3hwAAACAUOE16UkJEueruExnbQAAALgBocJj4uJ8B5pAESoAAADgAoQKD6KzNgAAANyEUOFBmWkVE+DRWRsAAABuQKjwIGoqAAAA4CaECk8PK8sEeAAAALCPUOFBWZWhgtGfAAAA4AaECg+i+RMAAADchFDhQVmVE+AxpCwAAADcgFDhQRnUVAAAAMBFCBWe7qhNqAAAAIB9hAoPoqM2AAAA3IRQ4UGZlX0qaP4EAAAANyBUeFBWauWM2oV+cRzH9uYAAAAgxhEqPNynoqzckfySMtubAwAAgBhHqPCglMQ4SUqoKDpm1QYAAEBMh4qlS5fKmDFjpEOHDuLz+WTu3Ln1rr948WKzXs3Tzp07JZbo/8wEeAAAAHALq6EiPz9fjjvuOHn88ccP6X4bN26UHTt2BE9t2rSRWBMMFQwrCwAAAMsSbD75OeecY06HSkNEVlaWxLLAsLLUVAAAAMA2T/apOP7446V9+/YyfPhw+fDDDyUW0fwJAAAAbmG1puJQaZCYMWOGnHjiiVJcXCzPPPOMnHnmmfLxxx/LCSecUOt9dD09BeTm5ppzv99vTrUJLK/rdjfISIk35z/mFbl6Ow+XF8ogmrH/7aMM7KMM7KMM7KMM7PK7YP+H+tw+xyUTHWjn4zlz5si4ceMO6X5Dhw6VLl26yAsvvFDr7ZMmTZLJkycftHz27NmSlpYmXvXPLXGyZGecDOtQLmO6ltveHAAAAEShgoICufjiiyUnJ0cyMjKio6aiNieffLIsW7aszttvv/12ufHGG6vVVHTu3FlGjBhR547RRLZw4ULTvCoxsaKZkdt8/d7XsmTn19KqQxcZPbqPRBsvlEE0Y//bRxnYRxnYRxnYRxnY5XfB/g+08mmI50PFmjVrTLOouiQnJ5tTTVowDRVOKOvY0jK94n/KKy5z7TY2BTeXQSxg/9tHGdhHGdhHGdhHGcTu/k8M8Xmthoq8vDzZvHlz8PqWLVtMSGjZsqVp0qS1DN9//708//zz5vZHHnlEunfvLn379pWioiLTp+K9996TBQsWSKzJTKOjNgAAANzBaqhYuXKlnHXWWcHrgWZKEydOlFmzZpk5KLZt2xa8vaSkRG666SYTNLQ/RP/+/eXdd9+t9hixIis1yZxnFzKjNgAAAGI4VOjITfX1E9dgUdWtt95qThDJYEhZAAAAuIQn56mASFZl86dsZtQGAACAZYQKj09+t7+oVMrKXTEqMAAAAGIUocLjoULl0gQKAAAAFhEqPCoxPk6aJVXMqk2/CgAAANhEqIiC2gpCBQAAAGwiVHhYZlpgWFlCBQAAAOwhVHhYZmrFiMDUVAAAAMAmQkU0NH8qYAI8AAAA2EOoiIJZtampAAAAgE2ECg/LZAI8AAAAuAChwsMY/QkAAABuQKjwMEIFAAAA3IBQ4WFZgeZPhAoAAABYRKiIgpqKXEIFAAAALCJUREGooKM2AAAAbCJUeBhDygIAAMANCBVRUFNR6C+T4tIy25sDAACAGEWo8LDmKQni81VcprYCAAAAthAqPCwuzicZKXTWBgAAgF2ECo+jszYAAABsI1REyVwVNH8CAACALYQKj6OmAgAAALYRKqIkVFBTAQAAAFsIFR5HqAAAAIBthAqPo08FAAAAbCNUeBw1FQAAALCNUBE1HbVLbG8KAAAAYhShwuMyU5PMOTUVAAAAsIVQ4XE0fwIAAIBthAqPo6M2AAAAbCNURFFNheM4tjcHAAAAMYhQESWhwl/mSEFJme3NAQAAQAwiVHhcWlK8JMb7zGWaQAEAAMAGQoXH+Xy+KsPKEioAAAAQeYSKKMAIUAAAALCJUBEFCBUAAACwiVARBbLSAhPgMas2AAAAIo9QEQWoqQAAAIBNhIooQEdtAAAA2ESoiALUVAAAAMAmQkUUIFQAAADAJkJFFMhKI1QAAADAHkJFFKCmAgAAADYRKqKopoKO2gAAALCBUBEFqKkAAACATYSKKJBRGSpyi/xSXu7Y3hwAAADEGEJFFNVUOI7I/qJS25sDAACAGEOoiALJCfGSmhhvLtMECgAAAJFGqIi2ztqFJbY3BQAAADHGaqhYunSpjBkzRjp06CA+n0/mzp3b4H0WL14sJ5xwgiQnJ0uPHj1k1qxZEdlWt6OzNgAAAGIyVOTn58txxx0njz/+eEjrb9myRc4991w566yzZM2aNXLDDTfI5ZdfLvPnz5dYF+iszbCyAAAAiLQEseicc84xp1DNmDFDunfvLg8++KC53rt3b1m2bJk8/PDDMnLkSIllWdRUAAAAwBJP9alYvny5DBs2rNoyDRO6PNbR/AkAAAAxWVNxqHbu3Clt27attkyv5+bmSmFhoaSmph50n+LiYnMK0HWV3+83p9oEltd1uxtlpFSM/rQ3r8hT210XL5ZBNGH/20cZ2EcZ2EcZ2EcZ2OV3wf4P9bk9FSoOx9SpU2Xy5MkHLV+wYIGkpaXVe9+FCxeKV+z8zici8bJh0xaZV/61RAsvlUE0Yv/bRxnYRxnYRxnYRxnE7v4vKCiIvlDRrl072bVrV7Vlej0jI6PWWgp1++23y4033litpqJz584yYsQIc7+6EpkW3vDhwyUxsaJZkdvt+2S7/Hv7F9K8VTsZPfp48TovlkE0Yf/bRxnYRxnYRxnYRxnY5XfB/g+08omqUDF48GCZN29etWW6o3V5XXToWT3VpAXTUOGEso5btExPMef7i0s9s82h8FIZRCP2v32UgX2UgX2UgX2UQezu/8QQn9dqR+28vDwzNKyeAkPG6uVt27YFaxkmTJgQXP/KK6+Ub775Rm699Vb58ssv5YknnpBXX31Vfv/730usC3TUZkhZAAAARJrVULFy5UoZMGCAOSltpqSX77zzTnN9x44dwYChdDjZf//736Z2Que30KFln3nmmZgfTrbqkLK5jP4EAACACLPa/OnMM88Ux3HqvL222bL1PqtXrw7zlnkPQ8oCAADAFk/NU4G6ZaVVhIr8kjLxl5Xb3hwAAADEEEJFlGiecqATDbUVAAAAiCRCRZSIj/NJ85SK1mx01gYAAEAkESqisAkUNRUAAACIJEJFVHbWLrG9KQAAAIghhIookpWaZM6pqQAAAEAkESqisaaCPhUAAACIIEJFFMms7FORTU0FAAAAIohQEUWYAA8AAAA2ECqiCM2fAAAAYAOhIopkUVMBAAAACwgVUYTmTwAAALCBUBFF6KgNAAAAGwgVUYSaCgAAANhAqIjSjtqO49jeHAAAAMQIQkUUyUqrmFG7pKxcivzltjcHAAAAMYJQEUWaJcVLfJzPXM4uLLG9OQAAAIgRhIoo4vP5GFYWAAAAEUeoiDJMgAcAAIBII1REGYaVBQAAQKQRKqIMw8oCAAAg0ggVUYbmTwAAAIg0QkWUoaM2AAAAIo1QEWVo/gQAAIBII1REmczKCfDoqA0AAIBIIVREGWoqAAAAEGmEiqjtqM2M2gAAAIgMQkWUaZ6SYM6/zy6U5V//KGXlju1NAgAAQJQjVESRd9bvkGtnf2Yu78krkfFPr5DT7nvPLAcAAADChVARJTQ4XPWPz0yYqGpnTpFZTrAAAABAuBAqooA2cZr85n+ktoZOgWV6O02hAAAAEA6EiijwyZa9siOnqM7bNUro7bqeF2j4+XjLXlm1x2fOmzoM6eNpf5M31nxPvxMAAIAmUNGrF562e3/dgaKqP81dJ0OOai1Ht2suR7dJl17tmktW5bwWodIDcA0n+pxtmqfIyd1bSnycT5qKNtPSWpWKkBQvz29aKe0zU+SuMX1kVL/2Tfz4FZry8SOxjwDADfisA1AVoSIK6Id5KL7+b745Vb9vsgkXR7dtLr3aNpeebdOlZ9vmkp6cEPED8kC/kJr1BoF+IdN/eUKjnifcjx94DkILmrK2rtWWvTK4RxvKGK4Sic86wO34Pq6OUBEF9EWsH+Z6cFxbQx59ebdKT5LbzzlGNu3Ol6927Ten7/YVyu79xeb0waY91e7TqUWqCRmmVqNtuvx3f7FMnfdl2A7IG+oXov+D3j68T7vDesOG+/GjKbREg3B/0Ifz8cNdWwc0ViQ+6wC34/v4YISKKKAHM/oi1g9zPayp+kEfOMz5v3H9DnqR5xWXyqbKgPHVrjxzvnHnfhMyNHDoadGXu+t97sBz3fzaWlmzPVvifL5qz+9UXnECS6vcGLjoOI78kF0YUr+QS/7+sbRIS5LS8nJzYFda7lScl1We11xeeZ5X7Jf/7i9p8PFHPLxEWjVLlsQEnyTFx0lSQpwkVp4nJ8SZZYHrgdt0eUK8Tx5euMnzoSVSIndQXsErTeiiqYy9jtoiez/QAG7HZ3XtCBVRQl+8+iKuebDTrp6DHW3iNKBLC3Oqal9+SbA2Q8PGp9/ulS937q/3+fOKy2TGkm8k3D76+sewPn5tTcSaQiC0DLh7gRzRPNn0ZclKTZTMtETJSk2SLD1PSzQzogduM8tSk8yEhnr/SH2Rh/tgyssH5eF8/EgerEVDlb3Xa4u8VptWXu7Irv1FMm/djpAHBhl8VCuJZV4r47qeg3BdHcG6boSKKKJfdvoibuyHTItmSXLKka3MSekoSde/vKbB+w09urUcdUTz4HVflaf11basypUd2YXy5ucNz6VxyaCu0qNNuvmfEuJ8Fefxeh5nrifUuB5Y74sdufLnNzY0+Pg3jzhajjwiXfxl5VJcWi4lpeXmsp6bk16uer3y9m9/zJc123MafPzcolJzEgk9uOhuSkuKl/zisga/yD/+5kcZ0qO1uLmjfDgPyif9q/6hle/61wYZdGQrSU6ID7424kJ8f4T6RTL06DbmdZFfUioFJWVSWFImBXrZX3E5v7hUCv26rOJUWLne1h8LQjpYe33Vdjm3f4da+z3FSpW9l4Opm2vTSsvK5fvsQvNa3Ppjvnxrzisub9tbYD4TQ/Xoe5vM5+KxHTNNvz2t1XXbAS01pofyHITrAP2eJVjXzudo25MYkpubK5mZmZKTkyMZGRm1ruP3+2XevHkyevRoSUxMlFinw67q7NwNeemKQYf9BtI3vs7+XV+/EK11WXbb/xx2n4pwPn6o++i+C4+VLi2bSU5hiWQX+CW70G/Og9crl+UUlJhzPdg8FLrtnVukSoesA6eOWSkHrmemSmpS/CEdTAX2RlP0m9EyqO/DuEVaotwxurcUlZZXHoyXSYG/NHg5cICuB+XB23W5v0z2F/nFX3boH2ca2uJ9vhpBNc405asaXDUo1LftkaY1Wdr3qVNWmnTUc3NKqzxPleYpiREv40gcLITjf9CvQX0N/ZhXIuc/8eFBk4hW1apZksz81UmSkZJown5acoKkJlaEVFvbfyiPP+3nx0vv9hmVgSG/IjTsrbj8/b5C02S0Lvo/tk5Pkl25xYe0TdpUtE/7DOnfKdOEjOM6Z8lRR1T8OBSNB+W2y7ipgm8knsOtZaw1cztzi0ww/nZPxftjy56K98s3e/JC+q65dEhXufqsHiEPplMXNxyThnLsrAgVLi1ANwn3AXnNDzGpo19IU30Qh+Pxw7WPikvLJKfQL0s3/ldufv1zaQotmyVJRxMyKsKGXm6XkWJ+xf8xv/aDqcD2L/j9GeYgfn9RqemTowfyeUWlst9cLjWXtf/K/spl5jZdp7hUfswrlh/z/RIrEuN95mAzLSlB0pL1PF7SEhNMqNPLgfNmSRXL/ru/SF78eHuDj9ssuf5aqwBtSqfhQstXw4aW9+Pvb5Z9BbWXQVO9j8N5sNBQMNWtbpuZIv+8aoh5De4rCIT1ipCu13MKKs4PhPgSs080NDZGSmJcRVlXlmttl1MS401NkzYXrYs2ffzjub0P9I+r0i9Nv631asX5get6wancPw8v/KqyNvTwaB+xLi3TpGurZtK1VZp0a5UmXVo1M+f6eaFhu77PusCPAxed2Fk2/JAjn3+XY8qiJn1v9OuoQSMrGDa6tWpmag69fFAeyms01PeZHp5pxtPHLDeXHXMwq33/6gt2R6Qnm+Crr4/a+xmWV+mHWGN5uT5HuTy0oP7XkYbLV34z2DTh1Sa6Wvt7KNxQxiP6tKs1OOh1DQ+HUjNXnyNbN5NTjmxpflg5pXsr8z46pHm1Nu+WBR98LCNOP8Va8zNCRR0IFe484PfCLxdeCC1aW7Qrt0h+yCmUH7KLTFMG7QSvv0Dqef4h1nzYoE0l9ABGDzpSqxyYmYPwyoP06gfmFet8uSNXfhdCM73nf32SnNitZcWXaFnFl6l+WR+4XvElXKbLqnzxfv5dtnntNOSZiSfK0KOPOOTmHocSTPVXdS3T7/YVVA6qEDivuFxXcAjFTwd2kr4dM00o0VNG5XngpL86h/NgwQysYJoJ+itOhRXBVA9MtQyeW75VwiXeJxJKZZceROkvmdqkzYvfoBqAtKZAD+K7VAaHQIho2zylwSaBh/JZp/tJm06t/S5b1n1XETLW/5BTay2s7td+HTLMOnV9VtV2UK7P4a88UNb3qjbj0teRv/I9rbeZ69qs1V8uVzy/ss4fUJS+zm8c3tMcwFecKpvB6nlpjet6e6CZbFm57Mkrli921N8HUWWkJJjt1+3S15B+3pjgUH7gspdeW/q50Dw5wZSh1pLquTbRDFw+cEo0n+OT3txQ7+dU2+bJ8v+uHlJt8Benxj6pGq7N9cp19DXwi6dXyH/rqXE0TV99IiX1vOF1nc4tD7w/ureueI9o6L746Y/Nd21d905LipeuLdPky137DypH/bFHw4UGjVO6tzSPV7U5uBubqhIq6kCoOHyReoGHO5l7tR1tY0OLvtX1AC0QNDR4VFwukvXf55hfaEKhu6rql0XF5QRJT0k0l/XLUs/TK79AArd/uydf7pi7PmzN6LzehK4pg6n229CyrRo2PtnyY0j9fhqiYa8ibCQcFDy0nGd9+G29v3CmJ8fLecd3kLyiiiZrum4gNOQW+psk+Poq+4ZVDHaQaEaM019UWwQGQdDbKpcHBknQARLWfZct45/+OOTXqL6n9NdM3d+Bpnim/0xtl4tLzYH1u1/UP6Ke6t2uualx0f9DDzaq90nzmfOK28y1inOfmPeyjsLXkGk/O17GDugotj7r9L30zX/zTHjQoPj59znynx9yD+mXYR2JT2lgiK2jmNAEPpv1s0h/4KjWvDN4Xrk8vvpyrQUJ5XWktVpN9Wu+LVWDQ7fWWiPXrPI8zdTyalPYxnxW5xT4zWA3n3y71/TFWP9Drnn9V6UtBUwthgkZreSoI5rJ/A07I9ZUNRSEijoQKsQTo8Z4uQy8GFpC7RMy89KT5MxeR9T6q0o0HZTbenw3lPFZvY4wzXS0lkCb3plTgd80Z4vkt4X+mq79FgK/bmpwKSktkxXf7G3wvrMvP+WwBixwS9+rww3Wkej/Fq7POv2lX0ccfGH5Vnn504abATZEN0MPCAMDeAQOrPV5QqnJO75zpvl1OjEwjHi8LziMeGBYcW3ieOD2ODMUuf448+iizQ0+/l8v7C8DumSZWiHt06W/yMfF6XZXHNhX7evlqzzX/0kPUCc++2lYy/hQXkda5sGmsJXNYAM/Ehw4VVwPrKf7KJRRFrXmUPePidVVgrQKhunKdYPh21cx4EChv+Gw8+f/7S0TB3erMziE47M6r7hUVm3dZwKGvnf0h4aa/TNaNUs0P64U1fE/NGVT1aYOFYz+hEOiL+BYG83ATfuoqUb4OpwJFPVD7IyjDy9QhDqfit7emP/lcIZWdtPjVy3jpq6tC7WMn5l4Uq3PFWiWFAwahRXNk6peX7s9O6Rhn0f1bScndmthgkJGIDRUayZRezOrUA/6AyPXue01GmoZ6HpufPxwftbpgXnfDpky9viOIYWKR39+vGnGqL+y6y/uFeeVl+sZ1S3UA+bbRvU+7BrT11Z+12AZXDiw02G9jk7rcUTYy/hQXkf6PwRqLEMVahn84/Lwhus+7TMPO1Ac7vdxenKCaR6rJ6UDjqzeriFjr3y85UdZvS27wb6Hbh5dilABeEw4QkskDvgjfVAertqicD++0sfStrY/fuGY86Z47MaWsTl40LlU0hLr/TIPJVRMHNLtsF7DXg+m4d7+SL2PwynUA1odVvlw/o9wB69oKGPCdeS+j1OT4mXIUa3NKTA4yxPvfy3TFm1q8L76/eM2hx/RAESVwMGUfthWpdebsv2mPo5W22rVuQ5vqed6vSnbhwY+6PVXTz1v6oOocD++V8s48GVe197Q5e0b+WUeiddp4DX6j1+fKBN6lpnzpnqNhnv7I/U+DpfAAa2q+TpqyoPycD1+tJRxOJ8jGso4XJIT4s1cSqFo7FC14UCfiihrzx8tKAN73DKEXawL53sg3P1+IjFSXCT6d3m1DCLx+OHm5ZEAo6mMw/l9EA1l7OVh/A8FfSoAuKbpDWKr30+4m7hFQ/+ucG+/1/dPuPoWRboZo9fLOJzfB9HQVDUc4j3cjJFQAQBoUl79Mkds/cDh9eAVDaIheHn5x5mo7FPx+OOPS7du3SQlJUVOOeUU+eSTT+pcd9asWRVDh1U56f0AAO7h1X4nAOAGo8LYtytqQ8Urr7wiN954o9x1113y2WefyXHHHScjR46U3bvrniBI23Pt2LEjeNq6NXwzrAIAAAC2ausGtvZGc2TroeKhhx6SK664Qn71q19Jnz59ZMaMGZKWlibPPvtsnffR2ol27doFT23bto3oNgMAAABwSagoKSmRVatWybBhww5sUFycub58+fI675eXlyddu3aVzp07y9ixY2XDhg0R2mIAAAAAruqovWfPHikrKzuopkGvf/nll7Xep1evXqYWo3///mZoqwceeECGDBligkWnTp0OWr+4uNicqg6LFRgqUE+1CSyv63aEH2VgF/vfPsrAPsrAPsrAPsrALr8L9n+oz211nooffvhBOnbsKB999JEMHjw4uPzWW2+VJUuWyMcffxzSP9q7d28ZP368TJky5aDbJ02aJJMnTz5o+ezZs00zKwAAAAC1KygokIsvvtjd81S0bt1a4uPjZdeuXdWW63XtKxEKnZBowIABsnnz5lpvv/32201H8Ko1FdpsasSIEfVOfrdw4UIZPnw4E69ZQhnYxf63jzKwjzKwjzKwjzKwy++C/R9o5dMQq6EiKSlJBg4cKIsWLZJx48aZZeXl5eb6tddeG9JjaPOpdevWmRlPa5OcnGxONWnBNFQ4oayD8KIM7GL/20cZ2EcZ2EcZ2EcZxO7+Twzxea1Pfqe1CBMnTpQTTzxRTj75ZHnkkUckPz/fjAalJkyYYJpITZ061Vy/++67ZdCgQdKjRw/Jzs6W+++/3wwpe/nll1v+TwAAAIDYZD1U/OxnP5P//ve/cuedd8rOnTvl+OOPl3feeSfYeXvbtm1mRKiAffv2mSFodd0WLVqYmg7tk6HD0QIAAACIwVChtKlTXc2dFi9eXO36ww8/bE4AAAAA3MH65HcAAAAAvI1QAQAAAMD7zZ8iKTAtR33DY+nwXTomr67DSAd2UAZ2sf/towzsowzsowzsowzs8rtg/weOmRua2i7mQsX+/fvNuc5VAQAAACC0Y+jMzEx3zqhtg86DoTN5N2/eXHw+X63rBCbI2759e70zByJ8KAO72P/2UQb2UQb2UQb2UQZ25bpg/2tU0EDRoUOHaiOySqzXVOjO6NSpU0jrauHxBrKLMrCL/W8fZWAfZWAfZWAfZRDb+z+znhqKADpqAwAAAGgUQgUAAACARiFU1CI5OVnuuusucw47KAO72P/2UQb2UQb2UQb2UQZ2JXto/8dcR20AAAAATYuaCgAAAACNQqgAAAAA0CiECgAAAACNQqio4fHHH5du3bpJSkqKnHLKKfLJJ5/Y3qSoMHXqVDnppJPMpINt2rSRcePGycaNG6utU1RUJNdcc420atVK0tPT5cILL5Rdu3ZVW2fbtm1y7rnnSlpamnmcW265RUpLSyP830SHe++910wAecMNNwSXUQbh9/3338svf/lLs49TU1Pl2GOPlZUrVwZv125ud955p7Rv397cPmzYMNm0aVO1x9i7d6/84he/MGOWZ2VlyWWXXSZ5eXkW/hvvKSsrkz//+c/SvXt3s3+POuoomTJlitnvAZRB01q6dKmMGTPGTJylnzlz586tdntT7e/PP/9cTj/9dPP9rZOF/fWvf43I/+f1MvD7/XLbbbeZz6JmzZqZdSZMmGAmCq6KMgjfe6CqK6+80qzzyCOPeG//a0dtVHj55ZedpKQk59lnn3U2bNjgXHHFFU5WVpaza9cu25vmeSNHjnRmzpzprF+/3lmzZo0zevRop0uXLk5eXl5wnSuvvNLp3Lmzs2jRImflypXOoEGDnCFDhgRvLy0tdfr16+cMGzbMWb16tTNv3jyndevWzu23327pv/KuTz75xOnWrZvTv39/5/rrrw8upwzCa+/evU7Xrl2dSy+91Pn444+db775xpk/f76zefPm4Dr33nuvk5mZ6cydO9dZu3atc9555zndu3d3CgsLg+uMGjXKOe6445wVK1Y4H3zwgdOjRw9n/Pjxlv4rb7nnnnucVq1aOW+99ZazZcsW57XXXnPS09OdadOmBdehDJqWfk7ccccdzj//+U9Nbs6cOXOq3d4U+zsnJ8dp27at84tf/MJ8z7z00ktOamqq8+STT0b0f/ViGWRnZ5vP9FdeecX58ssvneXLlzsnn3yyM3DgwGqPQRmE7z0QoLfrPu7QoYPz8MMPO17b/4SKKvRNdM011wSvl5WVmYKdOnWq1e2KRrt37zZvrCVLlgQ/1BITE80XfMAXX3xh1tEPuMCbMi4uztm5c2dwnenTpzsZGRlOcXGxhf/Cm/bv3+/07NnTWbhwoTN06NBgqKAMwu+2225zTjvttDpvLy8vd9q1a+fcf//9wWVaLsnJyeYLQv3nP/8xZfLpp58G13n77bcdn8/nfP/992H+D7zv3HPPdX79619XW3bBBReYL2JFGYRXzQOqptrfTzzxhNOiRYtqn0P6fuvVq1eE/jPvqO+gtuoPT7re1q1bzXXKIPz7/7vvvnM6duxoAoH++FQ1VHhl/9P8qVJJSYmsWrXKVLsGxMXFmevLly+3um3RKCcnx5y3bNnSnOu+1yrYqvv/mGOOkS5dugT3v55r9Wzbtm2D64wcOVJyc3Nlw4YNEf8fvEqbN2nzpar7WlEG4fevf/1LTjzxRLnoootM07EBAwbI008/Hbx9y5YtsnPnzmplkJmZaZpiVi0DrfrWxwnQ9fXz6uOPP47wf+Q9Q4YMkUWLFslXX31lrq9du1aWLVsm55xzjrlOGURWU+1vXeeMM86QpKSkap9N2sx23759Ef2fouU7Wpvg6H5XlEF4lZeXyyWXXGKaE/ft2/eg272y/wkVlfbs2WPa2lY9WFJ6XT/w0LRvHm3Hf+qpp0q/fv3MMt3H+kYIfIDVtv/1vLbyCdyGhr388svy2WefmT4uNVEG4ffNN9/I9OnTpWfPnjJ//ny56qqr5He/+50899xz1fZhfZ9Deq6BpKqEhAQT0CmDhv3hD3+Qn//85yYwJyYmmmCnn0faVllRBpHVVPubz6amo33rtI/F+PHjTft9RRmE13333Wf2p34f1MYr+z8hIs8C1PilfP369ebXQUTO9u3b5frrr5eFCxeaTlywE6j1l6a//OUv5roe0Op7YcaMGTJx4kTbmxcTXn31VXnxxRdl9uzZ5hfBNWvWmFChHSgpA8Q6ra3+6U9/ajrP6w8gCL9Vq1bJtGnTzA9+WjvkZdRUVGrdurXEx8cfNNKNXm/Xrp217Yo21157rbz11lvy/vvvS6dOnYLLdR9rE7Ts7Ow697+e11Y+gdvQ8AfX7t275YQTTjC/cOhpyZIl8uijj5rL+osGZRBeOrpNnz59qi3r3bu3GVGr6j6s73NIz7Ucq9LRt3RkEMqgYdq8IFBboU35tMnB73//+2DtHWUQWU21v/lsarpAsXXrVvPjU6CWQlEG4fPBBx+YfatNjQPfzVoGN910kxmN1Ev7n1BRSZt9DBw40LS1rfqrol4fPHiw1W2LBvqrhwaKOXPmyHvvvWeGc6xK9702Rai6/7UdoB5sBfa/nq9bt67aGyvwwVfzQA0HO/vss83+019mAyf91VybfQQuUwbhpU3+ag6lrG37u3btai7r+0I//KuWgfZX0TazVctAg5+GxAB9T+nnlbZDR/0KCgpMO+Sq9Acl3X+KMoisptrfuo4O26kHxlU/m3r16iUtWrSI6P/k5UChQ/m+++67ZsjrqiiD8LnkkkvMULBVv5u15lR/ANFmsp7a/xHrEu6RIWV1xIlZs2aZnva/+c1vzJCyVUe6weG56qqrzJCBixcvdnbs2BE8FRQUVBvOVIeZfe+998xwpoMHDzanmsOZjhgxwgxL+8477zhHHHEEw5k2QtXRnxRlEF46okpCQoIZ1nTTpk3Oiy++6KSlpTn/+Mc/qg2vqZ87b7zxhvP55587Y8eOrXV4zQEDBphhaZctW2ZG82I409BMnDjRjLASGFJWh3DUYZFvvfXW4DqUQdOPOKdDUOtJDzseeughczkwslBT7G8dMUqH07zkkkvM6Dn6fa7vLYYzbbgMSkpKzDC+nTp1Mp/rVb+jq44kRBmE7z1QU83Rn7yy/wkVNfztb38zB1U6X4UOMavjAaPx9E1U20nnrgjQL5Crr77aDImmb4Tzzz/ffKhV9e233zrnnHOOGXtZDwRuuukmx+/3W/iPojNUUAbh9+abb5pgpj9gHHPMMc5TTz1V7XYdYvPPf/6z+XLQdc4++2xn48aN1db58ccfzZeJzq+gw/n+6le/Ml9aaFhubq55zevnfEpKinPkkUea8eOrHjxRBk3r/fffr/XzXwNeU+5vneNCh2zWx9DgqGEFDZeBhuu6vqP1fgGUQfjeA6GECi/sf5/+iUydCAAAAIBoRJ8KAAAAAI1CqAAAAADQKIQKAAAAAI1CqAAAAADQKIQKAAAAAI1CqAAAAADQKIQKAAAAAI1CqAAAAADQKIQKAEDIunXrJo888kjI6y9evFh8Pp9kZ2eHdbsAAHYRKgAgCumBfH2nSZMmHdbjfvrpp/Kb3/wm5PWHDBkiO3bskMzMTAm3p59+Wo477jhJT0+XrKwsGTBggEydOjV4+6WXXirjxo0L+3YAQCxKsL0BAICmpwfyAa+88orceeedsnHjxuAyPfAOcBxHysrKJCGh4a+EI4444pC2IykpSdq1ayfh9uyzz8oNN9wgjz76qAwdOlSKi4vl888/l/Xr14f9uQEA1FQAQFTSA/nASWsJtHYicP3LL7+U5s2by9tvvy0DBw6U5ORkWbZsmXz99dcyduxYadu2rQkdJ510krz77rv1Nn/Sx33mmWfk/PPPl7S0NOnZs6f861//qrP506xZs0wtwvz586V3797meUaNGlUtBJWWlsrvfvc7s16rVq3ktttuk4kTJ9Zby6DP+dOf/lQuu+wy6dGjh/Tt21fGjx8v99xzj7lda2aee+45eeONN4K1Nbptavv27ea++nwtW7Y0++Dbb789qIZj8uTJJlRlZGTIlVdeKSUlJU1SVgAQDQgVABCj/vCHP8i9994rX3zxhfTv31/y8vJk9OjRsmjRIlm9erU52B8zZoxs27at3sfRg209KNeaAb3/L37xC9m7d2+d6xcUFMgDDzwgL7zwgixdutQ8/s033xy8/b777pMXX3xRZs6cKR9++KHk5ubK3Llz690GDUsrVqyQrVu31nq7Pr5uYyDA6EmbZvn9fhk5cqQJWR988IF5vkDQqRoadJ/oftIg8tJLL8k///lP838DACo5AICoNnPmTCczMzN4/f3333f043/u3LkN3rdv377O3/72t+D1rl27Og8//HDwuj7On/70p+D1vLw8s+ztt9+u9lz79u0Lbote37x5c/A+jz/+uNO2bdvgdb18//33B6+XlpY6Xbp0ccaOHVvndv7www/OoEGDzGMfffTRzsSJE51XXnnFKSsrC66jy2o+xgsvvOD06tXLKS8vDy4rLi52UlNTnfnz5wfv17JlSyc/Pz+4zvTp05309PRqjw8AsYyaCgCIUSeeeGK161pTob/oa7MkbQqkv9jrr/MN1VRoLUdAs2bNTPOg3bt317m+NpM66qijgtfbt28fXD8nJ0d27dolJ598cvD2+Ph400yrPvoYy5cvl3Xr1sn1119vmlBpkymtcSgvL6/zfmvXrpXNmzebmgr9f/WkTaCKiopMc7AA7QCu2x0wePBgs7+06RQAgI7aABCzNABUpYFi4cKFpmmS9ktITU2Vn/zkJw32HUhMTKx2Xfsr1HcgX9v6FZUejdevXz9zuvrqq02/h9NPP12WLFkiZ511Vq3razDQwKLNrRrbKR0AYhmhAgBgaH8C7ZSsna4DB9xVOyxHgnYq147iOnTtGWecYZbpyFSfffaZHH/88Yf0WH369DHn+fn5wZGo9LGqOuGEE8zoWG3atDE1LPXVaBQWFpqgpbT/htZqdO7c+ZD/RwCIRjR/AgAYOnKTdkBes2aNOYi++OKL661xCJfrrrvOzC+hIzXpMLjanGnfvn2mRqMuV111lUyZMsUEI+2srQf9EyZMMLUN2lQpMHKVdibXx9yzZ4/ppK2dylu3bm1GfNKO2lu2bDGdsXX0qe+++y74+FpboyNL/ec//5F58+bJXXfdJddee63ExfE1CgCKT0MAgPHQQw9JixYtzKhIOuqTjoqkv+RHmg4hq8PBaijQQKA1ArotKSkpdd5n2LBhJkhcdNFFcvTRR8uFF15o1tdRm3RYWnXFFVdIr169TF8SDRsaQLSfhI5A1aVLF7ngggtMfxIND9qnomrNxdlnn21Cl9ae/OxnP5PzzjvvsCcQBIBo5NPe2rY3AgCAumhtiR7s65CwWhsRadokTOfZaGhYWwCIZfSpAAC4ijZfWrBgQXBm7Mcee8w0S9LmWAAAd6L5EwDAVbSfgs68rTN6n3rqqWaYWJ3ZW2srAADuRPMnAAAAAI1CTQUAAACARiFUAAAAAGgUQgUAAACARiFUAAAAAGgUQgUAAACARiFUAAAAAGgUQgUAAACARiFUAAAAAGgUQgUAAAAAaYz/D0+ZyCb8DSroAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract steps and losses from trainer's log history\n",
    "steps = []\n",
    "losses = []\n",
    "\n",
    "for log in trainer.state.log_history:\n",
    "    if \"loss\" in log and \"step\" in log:\n",
    "        steps.append(log[\"step\"])\n",
    "        losses.append(log[\"loss\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(steps, losses, marker=\"o\", linestyle=\"-\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7ad39",
   "metadata": {},
   "source": [
    "Clearly for more efficient training in the future we can stop after 300 steps or so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc400c00",
   "metadata": {},
   "source": [
    "## Fine-Tuning using Layer Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60971583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 109,580,288 / 1,100,048,384 (9.96%)\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers in the base model transformer\n",
    "for param in base_model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last N transformer layers\n",
    "N = 1  # lets try unfreezing the last 2 layers\n",
    "for layer in base_model.model.layers[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Unfreeze the final output (language modeling) head\n",
    "for param in base_model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Print how many parameters will be updated\n",
    "trainable = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in base_model.parameters())\n",
    "print(f\"Trainable: {trainable:,} / {total:,} ({trainable / total:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d73350",
   "metadata": {},
   "source": [
    "I expect this to be slower than LoRA since its updating more of the model (10%) but lets see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84187cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/4zf8zzj14kx2rpg0wjq7rdh80000gn/T/ipykernel_30114/3583897076.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../.cache/historian_model_frz\",  # where to save the model\n",
    "    # batch size controls the number of samples processed in parallel...bigger batches have more stable gradients but require more memory so more GPU power would allow larger batches. Since we are working on a CPU/low memory situation and its a fairly small model we'll try 1-4\n",
    "    per_device_train_batch_size=4,\n",
    "    # since we are using a pretrained model and small dataset (and no GPU), 1-3 epoch for fine-tuning are probably enough. Starting with 1 for quick test/diagnostics run is good\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,  # how often to log training progress\n",
    "    save_steps=500,  # how often to save the model\n",
    "    fp16=False,  # use mixed precision training for faster training on GPUs\n",
    "    save_total_limit=1,  # keep only the latest model checkpoint\n",
    "    report_to=\"none\",  # disable reporting to any logging service\n",
    "    # max_steps=300,  # past runs show that loss levels off after ~300 steps, so we can limit training to that (or use early stopping callback below)\n",
    "    metric_for_best_model=\"loss\",  # use loss as the metric to determine the best model\n",
    "    eval_strategy=\"steps\",  # evaluate the model every few steps\n",
    "    eval_steps=250,  # evaluate every X steps\n",
    "    greater_is_better=False,  # lower loss is better\n",
    "    load_best_model_at_end=True,  # load the best model at the end of training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,  # use the same dataset for evaluation\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=2)\n",
    "    ],  # stop training if no improvement after 3 steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ec9fd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yarik/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='251' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 251/2500 03:40 < 33:14, 1.13 it/s, Epoch 0.10/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='357' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 357/1250 10:50 < 27:11, 0.55 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# resume_from_checkpoint=True\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/trainer.py:2622\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2620\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2622\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/trainer.py:3095\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3093\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3095\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3096\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/trainer.py:3044\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3044\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3045\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   3047\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/trainer.py:4173\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4170\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4172\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4173\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4174\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4183\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/trainer.py:4368\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4365\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   4367\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 4368\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4369\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4370\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4372\u001b[0m )\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/trainer.py:4584\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   4583\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4584\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   4585\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m   4587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/trainer.py:3810\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3809\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3810\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    684\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    685\u001b[0m )\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:324\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    323\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 324\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    327\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:162\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 162\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/vc_projects/ML/deep-learning/.venv-py31013/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    # resume_from_checkpoint=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fcc6fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract steps and losses from trainer's log history\n",
    "steps = []\n",
    "losses = []\n",
    "\n",
    "for log in trainer.state.log_history:\n",
    "    if \"loss\" in log and \"step\" in log:\n",
    "        steps.append(log[\"step\"])\n",
    "        losses.append(log[\"loss\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(steps, losses, marker=\"o\", linestyle=\"-\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8317e2",
   "metadata": {},
   "source": [
    "Time and loss was similar to LoRA for this model and dataset size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py31013",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
